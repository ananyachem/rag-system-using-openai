{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed42f10-f71b-4a43-bc09-ca1354f88d20",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12a04-3acf-43f0-b984-d99790d4afff",
   "metadata": {},
   "source": [
    "## Setup the functions for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0d2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os, bibtexparser, pypdf, logging\n",
    "import chromadb\n",
    "import json\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04c17cf-85f5-4920-bea1-db9e480a4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", 'content': \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ])\n",
    "\n",
    "    model_response = completion.choices[0].message['content']\n",
    "    tokens_used = completion['usage']['total_tokens']\n",
    "\n",
    "    return model_response, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc24019-6bef-4580-a823-aff13d959e49",
   "metadata": {},
   "source": [
    "## Parse data from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37bf02b8-9880-450c-9505-4968d59dc61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 78 files:\n",
      "  File: 2023.findings-emnlp.620.pdf\n",
      "  File: 29728-Article Text-33782-1-2-20240324-3.pdf\n",
      "  File: 2024.acl-long.642.pdf\n",
      "  File: 2021.findings-emnlp.320.pdf\n",
      "  File: 2020.coling-main.207.pdf\n",
      "  File: 2202.01110v2.pdf\n",
      "  File: 2212.14024v2.pdf\n",
      "  File: 2024.emnlp-industry.66.pdf\n",
      "  File: 8917_Retrieval_meets_Long_Cont.pdf\n",
      "  File: NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf\n",
      "  File: NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf\n",
      "  File: NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf\n",
      "  File: 2023.acl-long.557.pdf\n",
      "  File: tacl_a_00605.pdf\n",
      "  File: 3637870.pdf\n",
      "  File: 2023.emnlp-main.495.pdf\n",
      "  File: 3626772.3657834.pdf\n",
      "  File: 2402.19473v6.pdf\n",
      "  File: 3626772.3657957.pdf\n",
      "  File: 2024.eacl-demo.16.pdf\n",
      "  File: 967_generate_rather_than_retrieve_.pdf\n",
      "  File: 23-0037.pdf\n",
      "  File: 2022.naacl-main.191.pdf\n",
      "  File: 2312.10997v5.pdf\n",
      "  File: 947_Augmented_Language_Models_.pdf\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"pypdf\").setLevel(logging.CRITICAL)\n",
    "\n",
    "data_path = 'data/'\n",
    "data = {}\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "print('Reading %i files:' % len(files))\n",
    "for f in files:\n",
    "    path = os.path.join(data_path, f)\n",
    "\n",
    "    # each datum will have at least these attributes\n",
    "    d = {'filepath': None, 'title': None, 'text': None}\n",
    "\n",
    "    # parse bibtex file, if exists\n",
    "    if path.endswith('.bib'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        bib = bibtexparser.load(open(path, 'r'))\n",
    "        if 'title' in bib.entries[0]:\n",
    "            d['title'] = bib.entries[0]['title']\n",
    "            data[path[:-4]] = d\n",
    "\n",
    "    # parse pdf text, if exists\n",
    "    if path.endswith('.pdf'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        print('  File: %s' % f)\n",
    "        text = ''\n",
    "        reader = pypdf.PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        d['filepath'] = path\n",
    "        d['text'] = text\n",
    "        data[path[:-4]] = d\n",
    "\n",
    "data = [d for d in data.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba057a9-2f40-487c-a6b3-1afc772e10fe",
   "metadata": {},
   "source": [
    "## Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e20b3a5-6912-4421-bb0f-565e4953af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving each paper's extracted text to a text file for manual cleaning\n",
    "# for d in data:\n",
    "#     text_filename = f\"{d['filepath'][:-4]}.txt\" \n",
    "#     with open(text_filename, 'w') as f:\n",
    "#         f.write(d['text'])\n",
    "#     print(f\"Saved extracted text for {d['filepath']} to {text_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06603a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 78 files:\n",
      "  File: 2212.14024v2.txt\n",
      "  File: 2024.emnlp-industry.66.txt\n",
      "  File: 8917_Retrieval_meets_Long_Cont.txt\n",
      "  File: 2202.01110v2.txt\n",
      "  File: 2020.coling-main.207.txt\n",
      "  File: NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.txt\n",
      "  File: NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.txt\n",
      "  File: NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.txt\n",
      "  File: 29728-Article Text-33782-1-2-20240324-3.txt\n",
      "  File: 2023.findings-emnlp.620.txt\n",
      "  File: 2024.acl-long.642.txt\n",
      "  File: 2021.findings-emnlp.320.txt\n",
      "  File: 23-0037.txt\n",
      "  File: 2022.naacl-main.191.txt\n",
      "  File: 2312.10997v5.txt\n",
      "  File: 967_generate_rather_than_retrieve_.txt\n",
      "  File: 947_Augmented_Language_Models_.txt\n",
      "  File: 2023.emnlp-main.495.txt\n",
      "  File: 2023.acl-long.557.txt\n",
      "  File: tacl_a_00605.txt\n",
      "  File: 3637870.txt\n",
      "  File: 3626772.3657957.txt\n",
      "  File: 2024.eacl-demo.16.txt\n",
      "  File: 3626772.3657834.txt\n",
      "  File: 2402.19473v6.txt\n",
      "Loaded 25 text files.\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/'\n",
    "data = {}\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "print(f'Reading {len(files)} files:')\n",
    "\n",
    "for f in files:\n",
    "    path = os.path.join(data_path, f)\n",
    "\n",
    "    d = {'filepath': None, 'title': None, 'text': None}\n",
    "\n",
    "    # Only process '.txt' files\n",
    "    if path.endswith('.txt'):\n",
    "        print(f'  File: {f}')\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        d['filepath'] = path\n",
    "        d['text'] = text\n",
    "        d['title'] = f[:-4] \n",
    "\n",
    "        data[path[:-4]] = d\n",
    "\n",
    "data = [d for d in data.values()]\n",
    "\n",
    "print(f'Loaded {len(data)} text files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063fc8f-0d55-499d-a112-2d9752b77821",
   "metadata": {},
   "source": [
    "## Chunk data and generate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdece85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_for_chunk(chunk):\n",
    "    prompt = f\"Based on the following text, generate meaningful questions that can be answered using the content of the text:\\n\\n{chunk}\\n\\nPlease generate questions related to the content of the text.\"\n",
    "    questions, tokens_used = prompt_model(prompt)\n",
    "    return questions, tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37942d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for 2212.14024v2: 1. What is the main focus of the research presented in the abstract?\n",
      "2. How does the proposed DSP framework differ from existing retrieval-augmented in-context learning methods?\n",
      "3. What components are involved in the DSP framework?\n",
      "4. What tasks does the DSP framework aim to address?\n",
      "5. What are the advantages of using the DEMONSTRATE – SEARCH – PREDICT (DSP) framework for knowledge-intensive tasks?\n",
      "6. How does the DSP framework break down problems for the language model and retrieval model?\n",
      "7. What types of questions have the novel DSP programs been written to answer?\n",
      "8. What is the significance of the early evaluations mentioned in the abstract?\n",
      "9. What are the key features of \"retrieve-then-read\" pipelines as referenced in the text?\n",
      "10. What does the text imply about the relationship between language models and retrieval models in the context of the DSP framework?\n",
      "Questions for 2024.emnlp-industry.66: 1. What is Retrieval Augmented Generation (RAG) and how does it relate to Large Language Models (LLMs)?\n",
      "2. Which recent large language models have demonstrated exceptional capabilities in understanding long contexts?\n",
      "3. What was the main aim of the comparison conducted in the study between RAG and long-context LLMs?\n",
      "4. How do RAG and long-context LLMs perform against various public datasets according to the study's benchmarks?\n",
      "5. What are the findings concerning the average performance of long-context LLMs compared to RAG when resources are sufficient?\n",
      "6. What is the distinguishing advantage of RAG over long-context LLMs as mentioned in the study?\n",
      "7. Can you explain what S ELF -ROUTE is and how it improves the use of RAG and long-context LLMs?\n",
      "8. What are the benefits of using S ELF -ROUTE in terms of computation cost and performance?\n",
      "9. How does the study contribute to the understanding of long-context applications of LLMs?\n",
      "10. What might be the implications of the findings for future development and usage of LLMs in processing lengthy contexts?\n",
      "Questions for 8917_Retrieval_meets_Long_Cont: 1. What recent trend in large language models (LLMs) is discussed in the abstract?\n",
      "2. What are the two main methods explored in the study for extending the context window of LLMs?\n",
      "3. How do retrieval-augmentation and long context windows compare in terms of effectiveness for downstream tasks?\n",
      "4. What two state-of-the-art pretrained LLMs were used in this research?\n",
      "5. What surprising finding did the research reveal regarding a 4K context window LLM with retrieval-augmentation?\n",
      "6. How does the performance of a retrieval-augmented LLM with a 4K context window compare to a finetuned LLM with a 16K context window?\n",
      "7. What advantage does using retrieval-augmentation provide to LLMs, regardless of their context window size?\n",
      "8. What was the best-performing model identified in the study?\n",
      "9. What implications do the findings have for computational efficiency in LLMs? \n",
      "10. What is the significance of positional interpolation in the context of long context tasks?\n",
      "Questions for 2202.01110v2: 1. What is the primary focus of the survey conducted in the paper?\n",
      "2. How does retrieval-augmented text generation differ from conventional generation models?\n",
      "3. What advantages does retrieval-augmented text generation offer in NLP tasks?\n",
      "4. Which specific tasks are reviewed in the paper related to retrieval-augmented text generation?\n",
      "5. What are some of the notable approaches to retrieval-augmented text generation mentioned in the paper?\n",
      "6. How has retrieval-augmented text generation performed in comparison to state-of-the-art benchmarks?\n",
      "7. What emerging technologies does retrieval-augmented text generation combine?\n",
      "8. What promising directions for future research in retrieval-augmented text generation are suggested in the paper?\n",
      "Questions for 2020.coling-main.207: 1. What are the key attributes identified in the study for review generation?\n",
      "2. Why is review generation considered a difficult subtask of natural language generation?\n",
      "3. How do models typically generate text when provided with only user ID, product ID, and rating?\n",
      "4. What limitation do models face when generating text based solely on attribute identifiers without descriptive information?\n",
      "5. How does the proposed method in the paper aim to improve the review generation process?\n",
      "6. What role do references play in the proposed framework for review generation?\n",
      "7. How does the inclusion of references change the nature of the review generation task?\n",
      "8. What are the benefits of treating the review generation problem as an instance of text-to-text generation?\n",
      "9. What challenges are mentioned regarding the selection of references from a large pool of texts?\n",
      "10. How do the authors propose to enrich the inductive biases of the given attributes in the review generation process?\n",
      "Questions for NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks: 1. What has been demonstrated about large pre-trained language models in terms of factual knowledge storage?\n",
      "2. How do large pre-trained language models perform on downstream NLP tasks when fine-tuned?\n",
      "3. What limitations do large pre-trained language models have regarding knowledge access and manipulation?\n",
      "4. How do task-specific architectures compare to large pre-trained models on knowledge-intensive tasks?\n",
      "5. What are some of the open research problems related to pre-trained language models mentioned in the text?\n",
      "6. What is a differentiable access mechanism, and how can it help improve language models?\n",
      "7. For which type of tasks have pre-trained models with explicit non-parametric memory been previously investigated?\n",
      "8. What is the focus of the exploration in the described research regarding retrieval-augmented generation (RAG)?\n",
      "9. In the context of RAG models, what constitutes the parametric memory?\n",
      "10. What type of memory is referred to as non-parametric in the discussion of RAG models?\n",
      "Questions for NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory: 1. What is the primary focus of the research presented in the abstract?\n",
      "2. How does traditional memory retrieval work in the context of text generation tasks?\n",
      "3. What limitation does the traditional approach for memory retrieval have?\n",
      "4. What is defined as the \"primal problem\" in the text?\n",
      "5. What is the novel framework proposed in this research called?\n",
      "6. How does the Selfmem framework improve upon traditional memory retrieval methods?\n",
      "7. What is meant by \"self-memory\" in relation to the Selfmem framework?\n",
      "8. How does the Selfmem framework create an unbounded memory pool?\n",
      "9. What role does the memory selector play in the Selfmem framework?\n",
      "10. What is the goal of using better memory in the context of text generation, as outlined in the abstract?\n",
      "11. How is the effectiveness of Selfmem evaluated according to the text?\n",
      "Questions for NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models: 1. What is the main contribution of the paper discussed in the abstract?\n",
      "2. What challenges do existing methods for theorem proving face, according to the text?\n",
      "3. How does LeanDojo facilitate research on machine learning methods for theorem proving?\n",
      "4. What features does LeanDojo offer as an open-source Lean playground?\n",
      "5. How does LeanDojo assist in premise selection for theorem proving?\n",
      "6. What is ReProver, and how does it enhance the theorem proving process?\n",
      "7. What resources are required to train the ReProver model?\n",
      "8. What kind of annotations does LeanDojo provide for the proofs it contains?\n",
      "9. Why is the ability to interact with the proof environment programmatically significant?\n",
      "10. In what way does LeanDojo address the issue of reproducibility in theorem proving research?\n",
      "Questions for 29728-Article Text-33782-1-2-20240324-3: 1. What is the primary focus of the research discussed in the abstract regarding Retrieval-Augmented Generation (RAG)?\n",
      "2. What are the main challenges identified in existing research concerning RAG and large language models (LLMs)?\n",
      "3. How many fundamental abilities required for RAG are analyzed in the paper, and what are they?\n",
      "4. What are the four fundamental abilities evaluated in relation to RAG and large language models?\n",
      "5. What is the purpose of the Retrieval-Augmented Generation Benchmark (RGB) established in this research?\n",
      "6. In which languages is the RGB corpus designed for evaluation?\n",
      "7. How does the RGB benchmark categorize the instances within it?\n",
      "8. What impact does the paper aim to evaluate concerning Retrieval-Augmented Generation's effect on large language models?\n",
      "9. Why is it important to systematically investigate the impact of RAG on different large language models?\n",
      "10. What might be the potential implications of identifying bottlenecks in the capabilities of RAG for various LLMs?\n",
      "Questions for 2023.findings-emnlp.620: 1. What is the main purpose of retrieval-augmented generation as discussed in the abstract?\n",
      "2. What limitations of large language models are addressed by retrieval-augmented generation?\n",
      "3. How do retrievers struggle according to the text?\n",
      "4. What recent work has been proposed to improve relevance modeling in retrieval?\n",
      "5. What does the method ITER-RETGEN aim to achieve?\n",
      "6. How does ITER-RETGEN differ from recent work that interleaves retrieval with generation?\n",
      "7. In what manner does ITER-RETGEN combine retrieval and generation?\n",
      "8. What role does a model's response to a task input play in the ITER-RETGEN method?\n",
      "9. How does iterative retrieval contribute to generating better responses in ITER-RETGEN?\n",
      "10. What type of queries do retrievers struggle to capture relevance for, according to the text?\n",
      "Questions for 2024.acl-long.642: 1. What is the primary goal of the method introduced in the paper?\n",
      "2. How does the novel method for obtaining text embeddings differ from existing methods?\n",
      "3. What kind of data is used to train the text embedding model in this research?\n",
      "4. How many training steps are required for the proposed method?\n",
      "5. What kind of models does the method leverage to generate synthetic data?\n",
      "6. How many languages are covered by the synthetic data generated for the text embedding tasks?\n",
      "7. What kind of loss is used during the fine-tuning of open-source decoder-only LLMs?\n",
      "8. Did the proposed method require labeled data for achieving strong performance in text embedding benchmarks?\n",
      "9. What improvements were observed when fine-tuning with a mixture of synthetic and labeled data?\n",
      "10. What are the advantages of using synthetic data over manually collected datasets based on the paper's findings?\n",
      "Questions for 2021.findings-emnlp.320: 1. What issue do state-of-the-art dialogue models often experience, according to the abstract?\n",
      "2. What is the primary focus of the work discussed in the abstract?\n",
      "3. What architectural approach is explored for addressing factual incorrectness in dialogue models?\n",
      "4. How are neural-retrieval-in-the-loop architectures used in the context of knowledge-grounded dialogue?\n",
      "5. What are the main components studied in the architectures for knowledge-grounded dialogue?\n",
      "6. What is the goal of the authors in studying various types of architectures?\n",
      "7. What performance outcome do the authors claim their best models achieve?\n",
      "8. What types of tasks do the models demonstrate state-of-the-art performance on?\n",
      "9. What conversational abilities do the models exhibit apart from knowledge grounding?\n",
      "10. How do the models perform in terms of generalization in knowledge-grounded dialogues?\n",
      "Questions for 23-0037: 1. What are the key advantages of retrieval-augmented models compared to traditional large language models?\n",
      "2. What is the main focus of the research presented in the abstract?\n",
      "3. How does Atlas perform on knowledge-intensive tasks with few training examples?\n",
      "4. What specific tasks were evaluated to assess the performance of Atlas?\n",
      "5. How does the performance of Atlas on Natural Questions compare to that of a 540B parameter model?\n",
      "6. What percentage accuracy did Atlas achieve on Natural Questions using only 64 examples?\n",
      "7. How does the document index impact the effectiveness of the Atlas model?\n",
      "8. What are the implications of using a smaller parameter count while still achieving high accuracy in specific tasks?\n",
      "9. Can the document index used by Atlas be updated, and if so, how easily?\n",
      "10. What does the research suggest about the relationship between parameter counts and performance in few-shot settings?\n",
      "Questions for 2022.naacl-main.191: 1. What is in-context learning in the context of natural language understanding?\n",
      "2. How does in-context learning utilize large pre-trained language models (LMs)?\n",
      "3. What role do training examples, referred to as prompts, play in in-context learning?\n",
      "4. What issue does the proposed method in the study aim to address regarding prompt selection?\n",
      "5. How does the new method for retrieving prompts leverage annotated data and a language model?\n",
      "6. What is the process for labeling training examples as positive or negative in the proposed method?\n",
      "7. What type of model is trained to retrieve training examples for in-context learning?\n",
      "8. On which types of tasks was the proposed approach evaluated, and what is the nature of these tasks?\n",
      "9. What is the significance of the probability estimation in the method proposed for retrieving prompts?\n",
      "10. What are the expected outcomes of using the proposed prompt retrieval method in in-context learning?\n",
      "Questions for 2312.10997v5: 1. What are some of the challenges faced by Large Language Models (LLMs)?\n",
      "2. What is Retrieval-Augmented Generation (RAG) and how does it address the challenges of LLMs?\n",
      "3. How does RAG improve the accuracy and credibility of knowledge-intensive tasks?\n",
      "4. What are the benefits of incorporating knowledge from external databases in LLMs?\n",
      "5. What are the different paradigms of RAG mentioned in the review paper?\n",
      "6. Can you explain the tripartite foundation of RAG frameworks?\n",
      "7. What are the three key components of the RAG framework?\n",
      "8. How does RAG facilitate continuous knowledge updates in language models?\n",
      "9. What technologies are considered state-of-the-art in the context of RAG?\n",
      "10. Why is the integration of domain-specific information important for LLMs?\n",
      "Questions for 967_generate_rather_than_retrieve_: 1. What are knowledge-intensive tasks, and why do they require access to a large amount of world or domain knowledge?\n",
      "2. What is the traditional approach to handling knowledge-intensive tasks, and how does it function?\n",
      "3. What is the novel method introduced in this paper for solving knowledge-intensive tasks?\n",
      "4. How does the generate-then-read (GENREAD) method differ from the retrieve-then-read pipeline?\n",
      "5. What role does the large language model play in the GENREAD approach?\n",
      "6. What is the purpose of generating contextual documents in the GENREAD method?\n",
      "7. How does the clustering-based prompting method contribute to the generation of documents?\n",
      "8. Why is it important to generate diverse documents that cover different perspectives in knowledge-intensive tasks?\n",
      "9. What is the expected outcome of using the GENREAD method compared to traditional methods?\n",
      "10. How does the GENREAD method aim to improve the recall of acceptable answers in knowledge-intensive tasks?\n",
      "Questions for 947_Augmented_Language_Models_: 1. What are the primary features that differentiate Augmented Language Models (ALMs) from standard language models (LMs)?\n",
      "2. How do ALMs enhance their reasoning skills?\n",
      "3. In what ways can ALMs utilize external tools?\n",
      "4. What is the significance of decomposing complex tasks into simpler subtasks in the context of ALMs?\n",
      "5. How can ALMs learn to combine reasoning and tool usage?\n",
      "6. What role does the missing tokens prediction objective play in the performance of ALMs?\n",
      "7. How do augmented language models expand their context processing abilities?\n",
      "8. What benchmarks do ALMs outperform compared to regular LMs?\n",
      "9. What methodologies are used to enable ALMs to leverage their augmentations?\n",
      "10. How does the survey review current advancements in Augmented Language Models?\n",
      "Questions for 2023.emnlp-main.495: 1. What is the main issue that large language models (LMs) face, according to the text?\n",
      "2. How do most existing retrieval augmented LMs operate in terms of information retrieval?\n",
      "3. What is the limitation of the retrieve-and-generate setup mentioned in the text?\n",
      "4. What approach does the work propose to enhance the generation of long texts?\n",
      "5. What does FLARE stand for in the context of the proposed method?\n",
      "6. How does FLARE utilize predictions during the text generation process?\n",
      "7. What are the benefits of active retrieval augmented generation over traditional methods?\n",
      "8. Why is it important to continually gather information throughout the text generation process?\n",
      "9. What is the general purpose of augmenting LMs with external knowledge resources? \n",
      "10. What does the text suggest about the future of language models and their ability to generate accurate content?\n",
      "Questions for 2023.acl-long.557: 1. What are Chains-of-Thoughts (CoT) in the context of large language models (LLMs)?\n",
      "2. What challenges do prompting-based LLMs face when generating multi-step question answering (QA) responses?\n",
      "3. How does the retrieval of external knowledge sources help LLMs in answering questions?\n",
      "4. Why is the one-step retrieve-and-read approach considered insufficient for multi-step question answering?\n",
      "5. What is the primary focus or objective of the proposed approach, IRCoT?\n",
      "6. How does IRCoT improve the relationship between retrieval and the reasoning process in multi-step QA?\n",
      "7. What kind of improvements were observed when using IRCoT with GPT3?\n",
      "8. On how many datasets was IRCoT tested, and which datasets were mentioned?\n",
      "9. What are the specific improvements in retrieval and downstream QA achieved by using IRCoT?\n",
      "10. Why is it important for the retrieval process in multi-step QA to depend on previously derived steps?\n",
      "Questions for tacl_a_00605: 1. What are Retrieval-Augmented Language Modeling (RALM) methods designed to improve in language modeling performance?\n",
      "2. How do RALM methods address the issue of factually inaccurate text generation?\n",
      "3. What is one of the advantages of using RALM approaches in terms of source attribution?\n",
      "4. What challenges do existing RALM approaches face regarding deployment?\n",
      "5. What alternative method does this paper propose for incorporating grounding documents into language models?\n",
      "6. How does In-Context RALM differ from traditional RALM approaches in terms of LM architecture?\n",
      "7. What is the main benefit of using In-Context RALM with off-the-shelf general-purpose retrievers?\n",
      "8. How do the gains from In-Context RALM vary with different model sizes and corpora?\n",
      "9. How can the document retrieval and ranking mechanism be customized for RALM according to the paper?\n",
      "10. What method does the paper suggest for implementing In-Context RALM without further training of the language model?\n",
      "Questions for 3637870: Certainly! Here are some meaningful questions based on the provided text:\n",
      "\n",
      "1. What is the primary focus of text retrieval research?\n",
      "2. How have retrieval models evolved over time?\n",
      "3. What are the two main types of retrieval methods mentioned in the text?\n",
      "4. What is a key point in designing effective retrieval models according to the text?\n",
      "5. How do pretrained language models (PLMs) contribute to text retrieval?\n",
      "6. What advantage do PLMs provide in learning text representations?\n",
      "7. What is meant by \"semantic matching\" in the context of text retrieval?\n",
      "8. How does dense retrieval differ from traditional retrieval approaches?\n",
      "9. Why is the representation of texts using dense vectors important in relevance modeling?\n",
      "10. What does the term \"latent representation space\" refer to in the context of text retrieval?\n",
      "Questions for 3626772.3657957: 1. What are the challenges in evaluating retrieval-augmented generation (RAG) systems mentioned in the abstract?\n",
      "2. Why are traditional end-to-end evaluation methods considered computationally expensive?\n",
      "3. How does the performance of a retrieval model correlate with the downstream performance of a RAG system?\n",
      "4. What is the novel evaluation approach proposed in the text, and what does it entail?\n",
      "5. How does the eRAG method utilize each document in the retrieval list within the RAG system?\n",
      "6. What is the significance of using downstream task ground truth labels in the eRAG evaluation approach?\n",
      "7. What types of metrics are employed to obtain document-level annotations in the eRAG approach?\n",
      "8. How are the document-level annotations aggregated in the eRAG evaluation method?\n",
      "9. What results were observed from the extensive experiments performed on various datasets using eRAG?\n",
      "10. In what way does eRAG achieve a higher correlation with downstream performance compared to traditional methods?\n",
      "Questions for 2024.eacl-demo.16: 1. What is RAGA S1?\n",
      "2. What components make up a Retrieval Augmented Generation (RAG) system?\n",
      "3. How does the integration of a retrieval module benefit LLM-based systems?\n",
      "4. What are some challenges associated with evaluating RAG architectures?\n",
      "5. What are the key dimensions to consider when evaluating RAG systems?\n",
      "6. How does RAGA S1 propose to evaluate these dimensions of RAG architectures?\n",
      "7. Why is it important to have a reference-free evaluation framework like RAGA S1?\n",
      "8. What is the potential impact of RAGA S1 on the evaluation cycles of RAG architectures? \n",
      "9. What types of metrics does RAGA S1 provide for evaluation?\n",
      "10. How does the evaluation framework help reduce the risk of hallucinations in LLMs?\n",
      "Questions for 3626772.3657834: 1. What is Retrieval-Augmented Generation (RAG) and how does it enhance Large Language Models?\n",
      "2. Why is RAG considered important in enterprise settings and domains with constantly refreshed knowledge?\n",
      "3. What are the two types of retrieval components mentioned in RAG systems, and what is their significance?\n",
      "4. What specific aspect of RAG systems is the focus of the comprehensive examination conducted by the authors?\n",
      "5. What factors are analyzed in relation to the retrieval strategy of RAG systems?\n",
      "6. Why do the authors argue that the retrieval component of RAG systems deserves increased attention from researchers?\n",
      "7. What is the main argument presented by the authors regarding the passages retrieved by Information Retrieval (IR) systems in RAG?\n",
      "8. How does the position and number of retrieved passages affect the retrieval strategy in RAG systems?\n",
      "9. What is a counter-intuitive finding mentioned in the abstract related to RAG systems?\n",
      "Questions for 2402.19473v6: 1. What are the main factors contributing to the evolution of Artificial Intelligence Generated Content (AIGC)?\n",
      "2. What challenges does AIGC currently face despite its advancements?\n",
      "3. What is Retrieval-Augmented Generation (RAG) and how does it relate to AIGC?\n",
      "4. How does RAG enhance the generation process in AIGC?\n",
      "5. What benefits does RAG provide in terms of accuracy and robustness for AIGC?\n",
      "6. How are RAG techniques classified in the context of AIGC scenarios according to the paper?\n",
      "7. What are the fundamental abstractions of the augmentation methodologies for retrievers and generators discussed in the paper? \n",
      "8. Why is updating knowledge a challenge for AIGC?\n",
      "9. What is the significance of high-quality datasets in the development of AIGC?\n",
      "10. In what ways does RAG address issues like long-tail data and data leakage in AIGC?\n"
     ]
    }
   ],
   "source": [
    "chunk_question_mapping = {}\n",
    "total_tokens = 0\n",
    "\n",
    "for entry in data:\n",
    "    text_chunk = entry['text'][:1000]\n",
    "    questions, tokens_used = generate_questions_for_chunk(text_chunk)\n",
    "    chunk_id = f\"{entry['title']}_chunk_{len(chunk_question_mapping)}\"\n",
    "    \n",
    "    chunk_question_mapping[chunk_id] = {\n",
    "        'chunk': text_chunk,\n",
    "        'questions': questions.split('\\n'),  \n",
    "    }\n",
    "    print(f\"Questions for {entry['title']}: {questions}\")\n",
    "    total_tokens += tokens_used    \n",
    "\n",
    "with open('chunk_question_mapping.json', 'w') as f:\n",
    "    json.dump(chunk_question_mapping, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91801-86ab-43ae-9ff1-1cd0891e82e5",
   "metadata": {},
   "source": [
    "## Build the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6575ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.Client(Settings(persist_directory=\"./chromadb\"))\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "collection = client.get_or_create_collection(\"questions_data\")\n",
    "\n",
    "with open('chunk_question_mapping.json', 'r') as f:\n",
    "    chunk_question_mapping = json.load(f)\n",
    "\n",
    "questions = []\n",
    "question_ids = []\n",
    "metadatas = []\n",
    "\n",
    "for chunk_id, data in chunk_question_mapping.items():\n",
    "    for i, question in enumerate(data['questions']):\n",
    "        question_id = f\"{chunk_id}_question_{i}\"\n",
    "        \n",
    "        questions.append(question)\n",
    "        question_ids.append(question_id)\n",
    "        metadatas.append({\"chunk_id\": chunk_id})\n",
    "\n",
    "question_embeddings = model.encode(questions)\n",
    "\n",
    "collection.add(\n",
    "    documents=questions,\n",
    "    embeddings=question_embeddings,\n",
    "    metadatas=metadatas,\n",
    "    ids=question_ids\n",
    ")\n",
    "\n",
    "print(\"Indexing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60878b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_db(query, collection, model, chunk_question_mapping, top_k=3):\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for metadata in results['metadatas'][0]:\n",
    "        chunk_id = metadata['chunk_id']\n",
    "        \n",
    "        if chunk_id in chunk_question_mapping:\n",
    "            retrieved_chunks.append(chunk_question_mapping[chunk_id]['chunk'])\n",
    "        else:\n",
    "            print(f\"Warning: {chunk_id} not found in chunk_question_mapping.\")\n",
    "    \n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edadfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, chunks):\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    prompt = f\"Answer the following query based on the provided context. Use exact sentences from the context. If the answer is not in the context, respond with 'IDK'.\\n\\nQuery: {query}\\n\\nContext: {context}\"\n",
    "    answer, tokens_used = prompt_model(prompt)\n",
    "    return answer, tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30541-75ca-4647-86b3-4af98d395c31",
   "metadata": {},
   "source": [
    "## Conduct experiments to evaluate user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05fb0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-queries.json', 'r') as f:\n",
    "    test_questions = json.load(f)\n",
    "\n",
    "test_queries = [q['query'] for q in test_questions]\n",
    "test_references = [q['answer'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36db0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev-questions.json', 'r') as f:\n",
    "    dev_questions = json.load(f)\n",
    "\n",
    "dev_queries = [q['query'] for q in dev_questions]\n",
    "dev_references = [q['answer'] for q in dev_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c97244",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rag_answers = []\n",
    "\n",
    "for query in test_queries:\n",
    "    retrieved_chunks = query_vector_db(query, collection, model, chunk_question_mapping)\n",
    "    answer, tokens_used = generate_answer(query, retrieved_chunks)\n",
    "    \n",
    "    test_rag_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3156e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rag_answers = []\n",
    "\n",
    "for query in dev_queries:\n",
    "    retrieved_chunks = query_vector_db(query, collection, model, chunk_question_mapping)\n",
    "    answer, tokens_used = generate_answer(query, retrieved_chunks)\n",
    "    \n",
    "    dev_rag_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d6968a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeanDojo is \"an open-source Lean playground consisting of toolkits, data, models, and benchmarks.\"',\n",
       " 'The primary contribution of the paper on Retrieval-Augmented Language Modeling (RALM) is that it \"considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM.\"',\n",
       " 'AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs.',\n",
       " 'LeanDojo helps with theorem proving by \"removing these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks.\" It \"extracts data from Lean and enables interaction with the proof environment programmatically.\" LeanDojo \"contains fine-grained annotations of premises in proofs, providing valuable data for premise selection—a key bottleneck in theorem proving.\" Using this data, \"we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library.\"',\n",
       " 'The benefits of In-Context RALM include: \"they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism,\" and \"In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora.\"',\n",
       " '\"Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness.\"',\n",
       " 'IDK',\n",
       " 'IDK',\n",
       " 'IDK']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rag_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2ae2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM.',\n",
       " 'IDK',\n",
       " 'The different metrics for evaluating a RAG system include \"the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, and the quality of the generation itself.\" Additionally, with RAGA S, \"we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations.\"',\n",
       " 'RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases.',\n",
       " 'IDK',\n",
       " 'The context describes two RAG solutions when an input question does not cover all of the necessary detail for long form generation:\\n\\n1. \"We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve.\"\\n\\n2. \"To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT.\"',\n",
       " 'IDK',\n",
       " 'IDK',\n",
       " 'IDK']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_rag_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6940da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.84\n",
      "Recall: 0.92\n",
      "F1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Test Queries\n",
    "\n",
    "P, R, F1 = scorer.score(test_rag_answers, test_references)\n",
    "\n",
    "print(f\"Precision: {P.mean():.2f}\")\n",
    "print(f\"Recall: {R.mean():.2f}\")\n",
    "print(f\"F1 Score: {F1.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bfa7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.73\n",
      "Recall: 0.75\n",
      "F1 Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Dev Queries\n",
    "\n",
    "P, R, F1 = scorer.score(dev_rag_answers, dev_references)\n",
    "\n",
    "print(f\"Precision: {P.mean():.2f}\")\n",
    "print(f\"Recall: {R.mean():.2f}\")\n",
    "print(f\"F1 Score: {F1.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b2185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
